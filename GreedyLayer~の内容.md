#Greeedy Layer-Wise Training of Deep Networks
1. 要約
深い複層ニューラルネットワークは多くのレベルの非線形性を持ち、それらによって簡潔で高度な非線形性と高度に変化する関数を潜在的に代表する。
しかし、近年、ランダムな初期化から始まる勾配に基づく最適化(頻繁に貧弱な結果を陥る)が現れるまでは、そのような深いネットワークの訓練方法が定かではなかった。
Hintonらが最近、Deep Belief Network(DBN)のための貪欲な層ごとの教師なし学習アルゴリズムを提案した。
DBNは多くの隠れ層(中間層)をもつ生成モデル
最適化問題において、私達はこのアルゴリズムを経験的に研究し、その成功をより理解するために相違点を調査し、入力が連続的な場合、または教師あり問題で予測される変数の入力の分布の構造が十分に明らかでない場合へと拡張する。

2. 導入
  - カーネルマシーンやグラフィックベースの多様体や半教師あり学習アルゴリズムのような、最新のノンパラメトリック(解析の対象データに一切の分布を仮定しない)機械学習アルゴリズムに対する近年の理論的分析は、一部の学習アルゴリズムの根本的な制約を提唱している。
  - (これらの機械学習アルゴリズムの一部には根本的な部分での制約があることを提唱している)
この問題はカーネルに基づいたアプローチにおいて、カーネルが'local'の場合、明らかである。
x-yの絶対値が大きくなると、K(x, y)は定数に収束する。
それらの分析は、'highly-varying function'(重要な領域に、膨大な量の'変量'をもつ関数)を学習することの難しさを示しており、それらは膨大な量の、ある区間における線形近似に代表される要素を必要とする。
要素の数は、入力変数の数に比例して指数関数的に増えるため、この問題は、広く知られる基本的なノンパラメトリック学習アルゴリズムの次元数が引き起こす悪影響に直結する。仮にそれらすべての要素の形に関係性がなければば、適切に一般化するには、各要素に十分なサンプルが必要である。
しかし、仮にそれらの形に関係性があり、互いに予想し合うことができれば、'非ローカル'学習アルゴリズムには、トレーニングセットでカバーしとぃない部分を一般化する可能性がある。
そのような能力は人工知能問題(映像、言語、会話、ロボットなどに関連する)のような複雑な領域での学習において必要であるように思える。
highly-varying functionを(少ないパラメータを使って)簡潔に示す方法の一つは、多くの非線形性の合成である。
例えば、d個の入力があるパリティ関数は、O(2^d)個の例とガウシアンSVMで示されるパラメータ、一層の隠れ層のニューラルネットワーク用のO(d^2)のパラメータ、d層の複層ネットワーク用のO(d)のパラメータ、再帰型ニューラルネットワーク用のO(1)のパラメータを要する。
このような膨大な量のパラメータを調整して学習を行うのは非現実的である。
しかし、近年まで深層ニューラルネットワークのトレーニングはあまりに難しすぎると考えられていた。経験的には、深いネットワークは、1,2層の中間層を持つネットワークと比べ、一般的に良くないことがわかっている。
このような良くない結果にもかかわらず、機械学習学会ではそのことはあまり報告されていない。
合理的な説明としては、ランダムな初期化によって始まる勾配最適化が頻繁に好ましくない結果にはまると思われる原因は深いネットワークにある。
Hinton, Osindero, Tehらは近年、Deep Belief Networks(DBN)のためのgreedy layer-wise unsupervised learning algorithm(多くの、隠れ因果変数の層を持つ生成モデル)を提唱した。
そのようなネットワークに対するトレーニング方法は深いネットワークのトレーニングにおける問題を解決する原理を有するかもしれない。
  - DBNの上位層は(入力の結果xを表す)より'抽象的な'概念を示すことを想定し、下位層はxから'低水準の特徴'を抽出することを想定する。
  - (上位層から下位層に向けて、情報の圧縮を行っている?)
それらはまず、簡単な概念を学習し、その後それらをより抽象的な概念を学習するように作り直す。この方法はまだ機械学習においてそれほど開拓されていないが、DSNのための、貪欲な層ごとの構造的な学習アルゴリズムの基礎になる。
この論文では、この方法の利点をよく理解するために、オートエンコーダやgreedy layer-wise 教師あり学習についても研究する。
DBN用の学習アルゴリズムの３つの方向性(1.貪欲法で一度に1つの層の事前学習を行うこと、2.入力から情報を保存するために各層に教師なし学習を用いること、3.重要性の絶対的な基準に関するネットワーク全体の微調整)が独立に重要であるという仮説を立てる。
しかし、私達が発見したように、DBNが教師あり問題の初期化に使われる場合、貪欲事前学習での目標に関する情報を使うことは有利になる(少なくとも最初のそうで)
まずDBNとその構成層である制限付きボルツマンマシン(RMB)を拡張し、入力の連続値をより自然に処理できるようにする。
次に、貪欲教師なし学習がもたらす恩恵をより理解するための実験を行う。
答えるべき基礎的な問題は、このアプローチは難解な最適化問題の手助けになるかどうかだ。
最後に、入力変数xによって与えられる入力分布が目的変数yの条件付き分布を十分に明らかにしていない場合の貪欲教師なし手法で起こる問題について議論する。

3. Deep Belief Nets

